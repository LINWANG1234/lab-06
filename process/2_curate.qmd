---
title: "Lab 06: Taming data"
author: "Lin Wang"
format: html
bibliography: ["../bibliography.bib", "../packages.bib"]
---

```{r}
#| label: setup
#| message: false

library(dplyr)        # data manipulation
library(knitr)        # dynamic report generation
library(readtext)     # text data import
library(readr)        # efficient data reading
library(qtalrkit)     # document the dataset
```

```{r}
#| label: acquire-data
#| message: false
#| echo: false
#| eval: false

#
get_compressed_data(
  url = "https://github.com/francojc/activ-es/raw/master/activ-es-v.02/corpus/plain.zip", # access and download the file named 'plain.zip'
  target_dir = "../data/original/actives",  # the specific directory where the downloaded file will be saved
  confirmed = TRUE
)
```

## Data

<!--

- Overview of the data source and the purpose of this script

-->

### Description

```{r}
#| label: tbl-data-origin-file
#| tbl-cap: "Data origin: Actives corpus"
#| tbl-colwidths: [30, 70]
#| message: false

# Read the CSV file
read_csv("../data/original/actives_do.csv") |>
  kable () # Display the data as a table using kable

```

- the name of the data: 

"ACTIV-ES: a comparable Spanish corpus comprised of film dialogue from Argentine, Mexican and Spanish productions (v.02)"

- the source of the data: 

"URL: https://github.com/francojc/activ-es and DOI: 10.5281/zenodo.1492613"

- the nature of the data: 

Sampling frame is "Spanish-language TV-film transcripts from Argentina, Mexico, and Spain", data collection dates are "1940s-2010s"

- the acquisition strategy that was used: 

It involves downloading the data from the provided URL:(https://github.com/francojc/activ-es) and Zenodo DOI (10.5281/zenodo.1492613)

- the format of the data: 

"Running text files (.run), Part of Speech tagged files (.pos), and EAGLES tagged files (.eagles)"

### Structure

- the relevant directories and data files

1. Directories:

'data/original/actives': This directory contains the downloaded and decompressed data files.

2. Data files:

  - Running text files (.run)
  - Part of Speech tagged files (.pos)
  - EAGLES tagged files (.eagles)

- the metadata and/ or variables to be organized

1. Metadata:

- Resource name: "ACTIV-ES: a comparable Spanish corpus comprised of film dialogue from  Argentine, Mexican, and Spanish productions (v.02)"

- Data source: URL (https://github.com/francojc/activ-es) and DOI (10.5281/zenodo.1492613)

- Data sampling frame: Spanish-language TV-film transcripts from Argentina, Mexico, and Spain

- Data collection date(s): 1940s-2010s

- Data format: Running text files (.run), Part of Speech tagged files (.pos), and EAGLES tagged files (.eagles)

- Data schema: File names reflect language code, country, year, title, type, genre (first genre listed in IMDb), and IMDb ID

- License: GNU General Public License v2.0

- Attribution: Jerid Francom. (2018). 

- ACTIV-ES: a comparable Spanish corpus comprised of film dialogue from Argentine, Mexican, and Spanish productions (activ-es-v.02) [Data set]. Zenodo. (https://doi.org/10.5281/zenodo.1492613)

2. Variables:

  - Language code
  - Country
  - Year
  - Title
  - Type
  - Genre
  - IMDb ID

- the relationships between the data elements

The data elements likely have relationships based on their attributes. In the context of the "ACTIV-ES" dataset, these relationships might include:

  - Each film or production can be associated with multiple dialogue transcripts.

  - Attributes such as language code, country, year, title, type, genre, and IMDb ID are associated with each film or production.

  - There could be associations between films from the same country, year, or genre.


- the idealized format for the curated dataset in a tabular format

| Language Code | Country   | Year | Title            | Type    | Genre    | IMDb ID      | Dialogue Transcript File                    |
|---------------|-----------|------|------------------|---------|----------|--------------|---------------------------------------------|
| ES            | Argentina | 1995 | Example Movie 1  | Feature | Drama    | tt123456789  | example_movie1_dialogue.run                 |
| ES            | Mexico    | 2000 | Example Movie 2  | Feature | Comedy   | tt987654321  | example_movie2_dialogue.run                 |
| ES            | Spain     | 2005 | Example Movie 3  | Short   | Romance  | tt246810121  | example_movie3_dialogue.run                 |


In this tabular format:

  - Language Code: The code representing the language of the film.
  - Country: The country where the film was produced.
  - Year: The release year of the film.
  - Title: The title of the film.
  - Type: The type of film (e.g., feature, short).
  - Genre: The genre of the film.
  - IMDb ID: The IMDb identifier of the film.
  - Dialogue Transcript File: The file containing the dialogue transcript associated with the film.

This format allows for a structured representation of the dataset, making it easy to understand and analyze the characteristics of each film within the corpus. Each row represents a unique film, and each column provides specific attributes or metadata associated with that film.

## Curate

<!-- Overview of the data curation process -->

Data curation process:

- Data Collection: Gather the raw data from various sources.

- Data Cleaning: Remove errors, duplicates, and inconsistencies from the data.

- Data Integration: Combine data from different sources if needed.

- Data Transformation: Organize and structure the data for analysis.

- Data Validation: Check the data for accuracy and completeness.

- Data Documentation: Document the steps taken to clean and prepare the data.

- Data Storage: Store the curated data in a secure and accessible format.

- Data Sharing: Share the curated data with others for analysis or use.

By following these steps, the data curation process ensures that the data is organized, reliable, and ready for analysis or other purposes.

### Read

<!-- This code block reads data from the ACTIV-ES dataset. ... -->

```{r}
#| label: read-data-actives
#| message: false

# Defines document variable names
doc_vars <-
  c("lang", "country", "year", "title", "type", "genre", "imdbid")

# Read text files, extracting document variables and loading data into a tibble
actives_tbl <-
  #  Read text data from files into R
  readtext(
    file = "../data/original/actives/*.run", # Specify the path to the directory containing text files
    docvarsfrom = "filenames", # Indicate document variables should be extracted from filenames
    docvarnames = doc_vars, # Specify names of document variables
    verbosity = 0 # Control verbosity during reading
  ) |>
  as_tibble() # Convert output to a tibble format

#  This line creates a table with the text from the files and information about each file
actives_tbl
```

### Tidy

<!-- This code block performs data tidying on the actives_tbl dataset. 
It is part of the data processing pipeline for the "ACTIV-ES" dataset.... -->

```{r} 
#| label: tidy-data-actives

#
actives_tbl <-
  actives_tbl |>
  mutate(
    doc_id = row_number() # This line creates a new column called doc_id and assigns to each row a unique identifier based on its position in the tibble
  )

# Refers to the existing tibble named 'actives_tbl'
actives_tbl
```

### Write

<!-- This code block saves the curated actives_tbl dataset as a CSV file named "actives_curated.csv" in the "../data/derived/" directory. ... -->

```{r}
#| label: write-data-actives

# Write data frames or tibbles to CSV files
write_csv(
  x = actives_tbl, # Specify the data to be written, in this case, the actives_tbl tibble
  file = "../data/derived/actives_curated.csv" # Specify the path and filename where the CSV file will be saved
)
```

## Documentation

<!-- Overview of the purpose and structure of the documentation -->

This section gives an overview of why and how the "ACTIV-ES" dataset is documented.

- The purpose of the documentation is to:

  - Explain what the dataset is about and where it comes from.
  - Describe what information is included in the dataset.
  - Help users understand how to access and use the dataset effectively.
  - Document any changes or adjustments made to the dataset.

- The documentation is structured into:

1. Introduction: Provides background information on the dataset's origin and importance.

2. Data Description: Describes the dataset's content, including its variables and what they represent.

3. Accessing Data: Instructions on how users can access the dataset.

4. Using the Data: Guidance on how to work with and interpret the dataset.

5. Data Preparation: Details any cleaning or formatting done to the dataset.

6. References: Lists sources and credits for the dataset.

### Data dictionary

<!-- This code creates a simple data dictionary for the curated actives_tbl dataset and saves it as a CSV file named "actives_curated_dd.csv". ... -->

```{r}
#| label: create-data-dictionary
#| message: false

#  Take the data (actives_tbl) and generate a data dictionary.
create_data_dictionary(
  data = actives_tbl, # Specify the data for which the data dictionary will be created
  file_path = "../data/derived/actives_curated_dd.csv" # Specify the path and filename where the data dictionary CSV file will be saved
)
```

<!--

Note:

You will need to open and edit the `actives_curated_dd.csv` file to add the descriptions for each variable.

-->


```{r}
#| label: tbl-data-dictionary-show
#| tbl-cap: "Data dictionary: Actives corpus"
#| message: false

# Read the CSV file containing the data dictionary using the read_csv() function from the readr package.
read_csv("../data/derived/actives_curated_dd.csv") |>
  kable() # Generate a Markdown-formatted table from the data frame
```

## Self-assessment

### What did you learn?

- Through this exercise, I learned about the process of curating datasets, which involves steps such as data collection, cleaning, integration, transformation, validation, documentation, and storage.

- I also learned how to use R and Quarto Markdown to write code blocks and descriptions, making documentation clearer and more informative.

### What did you find most/ least challenging?

- The most challenging part was ensuring that the descriptions were concise yet informative, conveying the purpose and functionality of each code block effectively.

- The least challenging part was understanding the code itself, as I'm getting familiar with R and Markdown syntax.

### What resources did you consult?

- I referred to the R documentation for specific functions and packages used in the code blocks.
  - [R Documentation](https://www.rdocumentation.org/)  
  
### What more would you like to know about curating datasets?

- I would like to explore more advanced techniques for data cleaning and transformation, especially for dealing with large and complex datasets. Additionally, I'm interested in learning about best practices for versioning and maintaining curated datasets over time to ensure data integrity and reproducibility.

- Resources I consulted:

  - A post from [GitHub](https://gist.github.com/BioSciEconomist/be83a5352c91c1e82a5d0846635fa483) 
  
  - Youtube video about [data cleaning](https://www.youtube.com/watch?v=HZeL27QQsXY)
  
  - Youtube video about [data transformation](https://www.youtube.com/watch?v=R_rmnsjgvOQ)

